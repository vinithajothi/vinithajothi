# -*- coding: utf-8 -*-
"""Phase 2 app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10MrVNn4yxbIeNPnfA1mcALZ4h1FU8ayO
"""

# Step 1: Upload the Dataset
from google.colab import files
uploaded = files.upload()

# Step 2: Load the Dataset
import pandas as pd

df = pd.read_csv("phase 2 dataset .csv")  # Replace with exact filename if different
df.head()

# Step 3: Data Exploration
df.info()
df.describe()
df.shape
df.columns

# Step 4: Check for Missing Values and Duplicates
print("Missing values:\n", df.isnull().sum())
print("\nDuplicate rows:", df.duplicated().sum())

# Step 5: Visualize a Few Features
import seaborn as sns
import matplotlib.pyplot as plt

# Example: visualizing grade distribution
sns.histplot(df['skin_peeling'], kde=True)
plt.title ("Distribution of skin_peeling")
plt.show()

# Step 6: Identify Target and Features
# Assume "final_grade" is the target column (replace if needed)
target = 'skin_peeling'
features = df.drop(columns=[target]).columns
print("Features:", features.tolist())
print("Target:", target)

# Step 7: Convert Categorical Columns to Numerical
categorical_cols = df.select_dtypes(include=['object']).columns
print("Categorical columns:", categorical_cols.tolist())

# Step 8: One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
df_encoded.head()

# Step 9: Feature Scaling
from sklearn.preprocessing import StandardScaler

X = df_encoded.drop(columns=[target])
y = df_encoded[target]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 10: Train-Test Split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 11: Model Building
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)

# Step 12: Evaluation
from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X_test)
print("R2 Score:", r2_score(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))

# Step 13: Make Predictions from New Input
# Example new input (must match feature format)
new_data = X.iloc[0:1]  # Simulating one sample
new_prediction = model.predict(new_data)
print("Predicted Final Grade:", new_prediction)

# Step 14: Convert to DataFrame and Encode
def preprocess_input(input_dict):
    input_df = pd.DataFrame([input_dict])
    input_df = pd.get_dummies(input_df)

    # Align with training data columns
    input_df = input_df.reindex(columns=X.columns, fill_value=0)

    scaled_input = scaler.transform(input_df)
    return scaled_input

# Step 15: Predict the Final Grade
def predict_grade(input_dict):
    processed = preprocess_input(input_dict)
    prediction = model.predict(processed)
    return prediction[0]

# Step 16: Deployment â€“ Building an Interactive App
!pip install gradio
import gradio as gr

# Step 17: Create a Prediction Function
def gradio_interface(**inputs):
    result = predict_grade(inputs)
    return f"ðŸŽ“ Predicted Final Grade: {result:.2f}"

# Step 18: Create the Gradio Interface
#Customize based on actual feature names and types
input_components = []

for col in X.columns:
    if 'gender' in col:
        input_components.append(gr.Radio(['Male','Female'],label=col))
    elif 'age' in col or 'score' in col:
        input_components.append(gr.Number(label=col))
    else:
        input_components.append(gr.Textbox(label=col)) # Changed inputs_components to input_components

gr.Interface(
    fn=gradio_interface, # Changed gradio_predict to gradio_interface
    inputs=input_components,
    outputs="text",
    title="ðŸŽ“ Student Performance Predictor"
).launch()